{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q tensorflow-recommenders","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow_recommenders as tfrs\nprint(f\"TensorFlow Recommenders version: {tfrs.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Load the data (you've already done this part)\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e2/train.csv\")\ntrain_extra = pd.read_csv(\"/kaggle/input/playground-series-s5e2/training_extra.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine datasets for more training data\nall_train = pd.concat([train, train_extra], ignore_index=True)\nprint(all_train.shape)\nall_train[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_train.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(all_train.drop(columns=[\"Price\"]).describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(all_train[\"Weight Capacity (kg)\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(all_train.Price.describe().to_frame().T)\n\nsns.histplot(all_train.Price)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, handle missing values\nif all_train['Weight Capacity (kg)'].isnull().any():\n    # Impute with the median\n    median_weight = all_train['Weight Capacity (kg)'].median()\n    all_train['Weight Capacity (kg)'].fillna(median_weight, inplace=True)\n\n# Round to nearest integer to capture the spikes\nall_train['Weight Capacity Int'] = all_train['Weight Capacity (kg)'].round().astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\nnumerical_features = ['Compartments', 'Weight Capacity Int']\ntarget = 'Price'\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = all_train.drop(['id', target, 'Weight Capacity (kg)'], axis=1)\ny = all_train[target]\n\n# Split the data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create preprocessing pipelines for categorical and numerical features\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Combine preprocessors\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Preprocess the data\nX_train_preprocessed = preprocessor.fit_transform(X_train)\nX_val_preprocessed = preprocessor.transform(X_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Get the input dimensions for the model\ninput_dim = X_train_preprocessed.shape[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(input_dim):\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(32, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(1)  # Output layer for regression\n    ])\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='mean_squared_error',\n        metrics=['mean_absolute_error', 'mean_squared_error']\n    )\n    \n    return model\n\n\ndef build_dcn_model(input_dim, num_cross_layers=2, projection_dim=None, activity_regularizer=1e-5, kernel_regularizeer=1e-3):\n    \"\"\"\n    Build a Deep & Cross Network model using TFRS implementation\n    \n    Args:\n        input_dim: Dimension of input features\n        num_cross_layers: Number of cross layers to use\n        projection_dim: If not None, uses low-rank approximation for cross layers\n    \"\"\"\n    # Input layer\n    inputs = keras.Input(shape=(input_dim,))\n    \n    # Cross Network\n    x_cross = inputs\n    for _ in range(num_cross_layers):\n        # Using the TFRS implementation of Cross layer\n        x_cross = tfrs.layers.dcn.Cross(\n            projection_dim=projection_dim,\n            diag_scale=0.1,  # Small value to improve stability\n            kernel_regularizer=keras.regularizers.l2(kernel_regularizeer),\n            bias_regularizer=keras.regularizers.l2(kernel_regularizeer)\n        )(x_cross)\n    \n    # Deep Net\n    x_deep = keras.layers.Dense(128, \n                               activation='relu',\n                               kernel_regularizer=keras.regularizers.l2(kernel_regularizeer),\n                               activity_regularizer=keras.regularizers.l1(activity_regularizer))(inputs)\n    x_deep = keras.layers.BatchNormalization()(x_deep)\n    x_deep = keras.layers.Dropout(0.5)(x_deep)\n    \n    x_deep = keras.layers.Dense(64, \n                               activation='relu',\n                               kernel_regularizer=keras.regularizers.l2(kernel_regularizeer))(x_deep)\n    x_deep = keras.layers.BatchNormalization()(x_deep)\n    x_deep = keras.layers.Dropout(0.4)(x_deep)\n    \n    x_deep = keras.layers.Dense(32, \n                               activation='relu',\n                               kernel_regularizer=keras.regularizers.l2(kernel_regularizeer))(x_deep)\n    x_deep = keras.layers.BatchNormalization()(x_deep)\n    x_deep = keras.layers.Dropout(0.3)(x_deep)\n    \n    # Combine the two networks\n    combined = keras.layers.Concatenate()([x_cross, x_deep])\n    \n    combined = keras.layers.Dense(32, \n                                 activation='relu',\n                                 kernel_regularizer=keras.regularizers.l2(kernel_regularizeer))(combined)\n    combined = keras.layers.Dropout(0.3)(combined)\n    \n    # Output layer\n    outputs = keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2(kernel_regularizeer))(combined)\n    \n    # Create the model\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    optimizer = keras.optimizers.Adam(\n        learning_rate=0.001,\n        clipnorm=1.0,  # Clip gradients by norm\n        amsgrad=True\n    )\n    \n    # Compile\n    model.compile(\n        optimizer=optimizer,\n        loss='mean_squared_error',\n        metrics=['mean_absolute_error', \"mean_squared_error\"]\n    )\n    \n    return model\n\n# Create the model (standard)\n# model = build_model(input_dim)\n# Create model, DCN\nmodel = build_dcn_model(input_dim)\n\n# Define callbacks for training\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_mean_squared_error',\n    mode='min',\n    patience=10,\n    restore_best_weights=True,\n    verbose=1\n)\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=5,\n    min_lr=0.0001\n)\n\nweight_decay_callback = keras.callbacks.LearningRateScheduler(\n    lambda epoch, lr: lr * 0.99  # Slight decay per epoch\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    X_train_preprocessed, y_train,\n    epochs=30,\n    batch_size=1024,\n    # steps_per_epoch=1000, # comment for full data\n    # validation_steps=1000,\n    validation_data=(X_val_preprocessed, y_val),\n    callbacks=[early_stopping, reduce_lr, weight_decay_callback,],\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to make predictions on test data\ndef predict_price(model, preprocessor, new_data):\n    # We need to apply the same preprocessing to test data\n    if 'Weight Capacity (kg)' in new_data.columns:\n        # Handle missing values\n        if new_data['Weight Capacity (kg)'].isnull().any():\n            median_weight = new_data['Weight Capacity (kg)'].median()\n            new_data['Weight Capacity (kg)'].fillna(median_weight, inplace=True)\n        \n        # Round to nearest integer\n        new_data['Weight Capacity Int'] = new_data['Weight Capacity (kg)'].round().astype(int)\n        \n        # Drop the columns we don't need\n        new_data = new_data.drop(['Weight Capacity (kg)'], axis=1)\n    \n    # Transform the data and predict\n    new_data_preprocessed = preprocessor.transform(new_data)\n    predictions = model.predict(new_data_preprocessed)\n    return predictions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/playground-series-s5e2/test.csv\")\nprint(\"Test shape\", test.shape )\ntest[:5]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = predict_price(model, preprocessor, test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/playground-series-s5e2/sample_submission.csv\")\nsub.Price = preds\nsub.to_csv(f\"submission.csv\",index=False)\nsub.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}